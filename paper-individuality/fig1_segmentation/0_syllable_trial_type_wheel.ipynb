{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '/home/ines/repositories/'\n",
    "# prefix = '/Users/ineslaranjeira/Documents/Repositories/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "IMPORTS\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from one.api import ONE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Get my functions\n",
    "from functions import idxs_from_files, state_identifiability, align_bin_design_matrix, states_per_trial_phase, broader_label\n",
    "from functions import define_trial_types, rescale_sequence, plot_binned_sequence\n",
    "\n",
    "one = ONE(mode='remote')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "data_path = prefix + 'representation_learning_variability/paper-individuality/data/design_matrices/'\n",
    "paw_wavelet_path = prefix + 'representation_learning_variability/paper-individuality/data/paw_wavelets/'\n",
    "wheel_wavelet_path = prefix + 'representation_learning_variability/paper-individuality/data/wheel_wavelets/'\n",
    "\n",
    "all_files = os.listdir(data_path)\n",
    "design_matrices = [item for item in all_files if 'design_matrix' in item and 'standardized' not in item]\n",
    "idxs, mouse_names = idxs_from_files(design_matrices)\n",
    "\n",
    "\"\"\" FITTING PARAMETERS \"\"\"\n",
    "num_iters = 100\n",
    "num_states = 2\n",
    "num_train_batches = 5\n",
    "method='prior'\n",
    "fit_method='em'\n",
    "threshold = 0\n",
    "\n",
    "\n",
    "optimal_k = 8\n",
    "optimal_k_wheel = 3\n",
    "\n",
    "whisker_params = str(num_train_batches)+'_'+method+'_'+fit_method+'_zsc_'+'True/'\n",
    "licking_params = str(num_train_batches)+'_'+method+'_'+fit_method+'_zsc_'+'False/'\n",
    "\n",
    "states_path = prefix + 'representation_learning_variability/paper-individuality/data/hmm/most_likely_states/'\n",
    "paw_wavelet_states_path = prefix + 'representation_learning_variability/paper-individuality/data/paw_most_likely_states/'\n",
    "wheel_wavelet_states_path = prefix + 'representation_learning_variability/paper-individuality/data/wheel_most_likely_states/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 218 sessions to process.\n"
     ]
    }
   ],
   "source": [
    "# Identify sessions available to process\n",
    "sessions_to_process = []\n",
    "for m, mat in enumerate(idxs):\n",
    "    mouse_name = mat[37:]\n",
    "    session = mat[:36]\n",
    "    fit_id = str(mouse_name + session)\n",
    "    whisker_filename = os.path.join(states_path + whisker_params, 'whisker_me' + '_' + fit_id)\n",
    "    licks_filename = os.path.join(states_path + licking_params, 'Lick count' + '_' + fit_id)\n",
    "    paw_wavelet_filename = os.path.join(paw_wavelet_states_path, \"most_likely_states_\" + str(optimal_k) + '_' + fit_id+'.npy')\n",
    "    wheel_wavelet_filename = os.path.join(wheel_wavelet_states_path, \"most_likely_states_\" + str(optimal_k_wheel) + '_' + fit_id+'.npy')\n",
    "\n",
    "    # if os.path.exists(whisker_filename) and os.path.exists(licks_filename) and os.path.exists(paw_wavelet_filename) and os.path.exists(wheel_wavelet_filename):\n",
    "    if os.path.exists(whisker_filename) and os.path.exists(licks_filename) and os.path.exists(paw_wavelet_filename):\n",
    "        sessions_to_process.append((mouse_name, session))\n",
    "\n",
    "print(f\"Found {len(sessions_to_process)} sessions to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifiable_mapping = {}\n",
    "\n",
    "counter = 0\n",
    "for wheel in [0, 1, 2]:\n",
    "    for lick in [0, 1]:\n",
    "        for whisk in [0, 1]:\n",
    "            for paw in range(8):\n",
    "                key = f\"{paw}{whisk}{lick}{wheel}\"\n",
    "                identifiable_mapping[key] = float(counter)\n",
    "                counter += 1\n",
    "\n",
    "identifiable_mapping[\"nan\"] = np.nan\n",
    "\n",
    "paw_fix_mapping = {0:4, 1:1, 2:5, 3:7, 4:6, 5:2, 6:0, 7:3}\n",
    "wheel_fix_mapping = {0:1, 1:0, 2:2} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wheel_over_wavelet_clusters(init, inter, empirical_data, session_trials, color_states):\n",
    "    # Plot raw trace over states\n",
    "    # init should be in seconds; inter should be in frames\n",
    "    frame_rate = 60\n",
    "    plot_min = -10\n",
    "    plot_max = 10\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=1 , nrows=1, sharex=False, sharey=False, figsize=[20, 5])\n",
    "    plt.rc('font', size=12)\n",
    "    palette = ListedColormap(sns.color_palette('Set2', n_colors=8))\n",
    "\n",
    "    # ax.plot(data)\n",
    "    ax.plot(empirical_data.loc[empirical_data['Bin']>=init, 'avg_wheel_vel'].reset_index(), color='black')\n",
    "    ax.plot(zscore(empirical_data.loc[empirical_data['Bin']>=init, 'l_paw_x'].reset_index(), nan_policy='omit'), color='green')\n",
    "    ax.plot(zscore(empirical_data.loc[empirical_data['Bin']>=init, 'r_paw_x'].reset_index(), nan_policy='omit'), color='red')\n",
    "    ax.imshow(np.concatenate([empirical_data.loc[empirical_data['Bin']>=init, color_states]])[None,:],\n",
    "                extent=(0, len(np.concatenate([empirical_data.loc[empirical_data['Bin']>=init, color_states]])), -10, 10),\n",
    "                aspect=\"auto\",\n",
    "                cmap=palette,\n",
    "                alpha=0.6)\n",
    "\n",
    "    for state, marker in zip([0.,1.], ['o','']):\n",
    "        mask_whisker = empirical_data.loc[empirical_data['Bin']>=init, 'whisker_me_states'] == state\n",
    "        masked_time_whisker = (empirical_data.loc[empirical_data['Bin']>=init, 'Bin'][mask_whisker].reset_index()['Bin']* frame_rate)-init*frame_rate\n",
    "        mask_lick = empirical_data.loc[empirical_data['Bin']>=init, 'Lick count_states'] == state\n",
    "        masked_time_lick = (empirical_data.loc[empirical_data['Bin']>=init, 'Bin'][mask_lick].reset_index()['Bin']* frame_rate)-init*frame_rate\n",
    "        ax.scatter(masked_time_whisker, np.zeros(np.sum(mask_whisker))+2, marker=marker, c='purple', s=20, alpha=.6)\n",
    "        ax.scatter(masked_time_lick, np.zeros(np.sum(mask_lick))+3, marker=marker, c='blue', s=20, alpha=.6)\n",
    "    \n",
    "\n",
    "    ax.vlines(np.array(session_trials['goCueTrigger_times'] -init)*frame_rate, plot_min, plot_max, label='Stim On', \n",
    "                color='Black', linewidth=2)\n",
    "    ax.vlines(np.array(session_trials.loc[session_trials['feedbackType']==1, 'feedback_times'] * frame_rate)-init*frame_rate, \n",
    "                plot_min, plot_max, label='Correct', color='Green', linewidth=2)\n",
    "    ax.vlines(np.array(session_trials.loc[session_trials['feedbackType']==-1, 'feedback_times'] * frame_rate)-init*frame_rate, \n",
    "                plot_min, plot_max, label='Incorrect', color='Red', linewidth=2)\n",
    "    ax.vlines(np.array(session_trials['firstMovement_times'] * frame_rate)-init*frame_rate, plot_min, plot_max, label='First movement', color='Blue')\n",
    "    ax.vlines(np.array(session_trials['intervals_0'] * frame_rate)-init*frame_rate, plot_min, plot_max, label='Trial end', color='Grey', linewidth=2)\n",
    "    ax.vlines(np.array((session_trials['goCueTrigger_times'] - session_trials['quiescencePeriod']) * frame_rate)-init*frame_rate, \n",
    "                plot_min, plot_max, label='Quiescence start', color='Pink', linewidth=2)\n",
    "\n",
    "    ax.set_xlim([init, init+inter])\n",
    "    ax.set_ylabel(\"Wheel velocity\")\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_xticks(np.arange(0, inter, inter/5),np.arange(init, \n",
    "                                                          (init+inter)/frame_rate, (inter/frame_rate)/5))\n",
    "    ax.set_title(\"Wavelet transform clusters\")\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    ax.set_ylim([-10, 10])\n",
    "\n",
    "    plt.tight_layout()    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_interest = ['paw', 'whisker_me', 'Lick count', 'wheel']\n",
    "path_sets = [paw_wavelet_states_path, states_path + whisker_params, states_path+licking_params, wheel_wavelet_states_path]\n",
    "\n",
    "states_trial_type = pd.DataFrame(columns=['mouse_name', 'session', 'correct', 'choice', 'contrast', \n",
    "                                          'reaction', 'response', 'elongation', 'most_likely_states', \n",
    "                                          'identifiable_states', 'Bin', 'label'])\n",
    "\n",
    "for m, mat in enumerate(sessions_to_process):\n",
    "\n",
    "    mouse_name = mat[0]\n",
    "    session = mat[1]\n",
    "\n",
    "    fit_id = str(mouse_name+session)\n",
    "    trials_file = data_path + \"session_trials_\" + str(session) + '_'  + mouse_name\n",
    "    session_trials = pd.read_parquet(trials_file, engine='pyarrow').reset_index() \n",
    "    \n",
    "    session_states = pd.DataFrame()\n",
    "    for v, var in enumerate(var_interest):\n",
    "\n",
    "        # Get most likely states filename\n",
    "        use_path = path_sets[v]\n",
    "        var_name = var_interest[v]\n",
    "        use_k = optimal_k if var=='paw' else optimal_k_wheel\n",
    "        states_filename = os.path.join(use_path, \n",
    "                                       f\"{'most_likely_states_'+str(use_k)+ '_' + fit_id + '.npy' if var_name=='paw' or var_name=='wheel' else var_name+ '_' + fit_id}\")\n",
    "        \n",
    "        column_name = var + '_states'\n",
    "        # Load design matrix used to obtain states and compare sizes\n",
    "        if var == 'wheel':\n",
    "            use_vars = ['avg_wheel_vel0.5', 'avg_wheel_vel1.0', 'avg_wheel_vel2.0', 'avg_wheel_vel4.0', 'avg_wheel_vel8.0', \n",
    "                            'avg_wheel_vel16.0', 'avg_wheel_vel32.0']   \n",
    "            filename = wheel_wavelet_path + \"wheel_vel_wavelets_\" + str(session) + '_'  + mouse_name\n",
    "            design_matrix = pd.read_parquet(filename)\n",
    "            most_likely_states = np.load(open(states_filename, \"rb\"))\n",
    "            # Rename states to be more intuitive\n",
    "            replace_func = np.vectorize(wheel_fix_mapping.get)\n",
    "            most_likely_states = replace_func(most_likely_states)\n",
    "            # not_nan_indices = ~np.isnan(np.array(design_matrix)).any(axis=1)\n",
    "\n",
    "            not_nan_indices = ~np.isnan(np.array(design_matrix[use_vars])).any(axis=1)\n",
    "            assert np.sum(not_nan_indices) == len(most_likely_states)\n",
    "            design_matrix[column_name] = design_matrix['Bin'] * np.nan\n",
    "            design_matrix[column_name].loc[not_nan_indices] = most_likely_states\n",
    "        \n",
    "        elif var == 'paw':\n",
    "            use_vars = ['l_paw_x0.5', 'l_paw_x1.0', 'l_paw_x2.0', 'l_paw_x4.0', 'l_paw_x8.0', \n",
    "                    'l_paw_y0.5', 'l_paw_y1.0', 'l_paw_y2.0', 'l_paw_y4.0', 'l_paw_y8.0', \n",
    "                    'r_paw_x0.5', 'r_paw_x1.0', 'r_paw_x2.0', 'r_paw_x4.0', 'r_paw_x8.0', \n",
    "                    'r_paw_y0.5', 'r_paw_y1.0', 'r_paw_y2.0', 'r_paw_y4.0', 'r_paw_y8.0']\n",
    "            filename = paw_wavelet_path + \"paw_vel_wavelets_\" + str(session) + '_'  + mouse_name\n",
    "            design_matrix = pd.read_parquet(filename)\n",
    "            most_likely_states = np.load(open(states_filename, \"rb\"))\n",
    "            # Rename states to be more intuitive\n",
    "            replace_func = np.vectorize(paw_fix_mapping.get)\n",
    "            most_likely_states = replace_func(most_likely_states)\n",
    "            not_nan_indices = ~np.isnan(np.array(design_matrix[use_vars])).any(axis=1)\n",
    "            assert np.sum(not_nan_indices) == len(most_likely_states)\n",
    "            design_matrix[column_name] = design_matrix['Bin'] * np.nan\n",
    "            design_matrix[column_name].loc[not_nan_indices] = most_likely_states\n",
    "\n",
    "        else:\n",
    "            most_likely_states, _, _ = pickle.load(open(states_filename, \"rb\"))\n",
    "            filename = paw_wavelet_path + \"paw_vel_wavelets_\" + str(session) + '_'  + mouse_name\n",
    "            design_matrix = pd.read_parquet(filename)\n",
    "            not_nan_indices = ~np.isnan(np.array(design_matrix)).any(axis=1)\n",
    "            num_timesteps = np.shape(design_matrix.dropna())[0]\n",
    "            hmm_length = (num_timesteps // num_train_batches) * num_train_batches\n",
    "            assert hmm_length == len(most_likely_states)\n",
    "            use_indices = not_nan_indices[:hmm_length]\n",
    "            design_matrix[column_name] = design_matrix['Bin'] * np.nan\n",
    "            rows = design_matrix.index[not_nan_indices][:hmm_length]\n",
    "            design_matrix.loc[rows, column_name] = most_likely_states\n",
    "\n",
    "        # Join data\n",
    "        if v == 0:\n",
    "            session_states = design_matrix[['Bin', 'Lick count', 'avg_wheel_vel', 'whisker_me', 'avg_whisker_me',\n",
    "       'l_paw_x', 'l_paw_y', 'r_paw_x', 'r_paw_y', column_name]]\n",
    "        else:\n",
    "            session_states = session_states.merge(design_matrix[['Bin', column_name]], on='Bin', how='outer')\n",
    "    \n",
    "    # Transform states into identifiable states\n",
    "    sets_to_identify = ['whisker_me', 'Lick count']\n",
    "    session_states = state_identifiability(session_states, sets_to_identify)\n",
    "\n",
    "    # Combine states\n",
    "    session_states = session_states.dropna() \n",
    "    c_states = np.array(session_states[['paw_states', 'whisker_me_states', 'Lick count_states', 'wheel_states']].astype(int))\n",
    "    combined_states = np.array([''.join(map(str, row)) for row in c_states])\n",
    "    session_states['identifiable_states'] = combined_states\n",
    "\n",
    "    # Back to integer\n",
    "    replace_func = np.vectorize(identifiable_mapping.get)\n",
    "    integer_states = replace_func(combined_states)\n",
    "    session_states['most_likely_states'] = integer_states\n",
    "    \n",
    "    # Get back the NaNs\n",
    "    final_states = design_matrix[['Bin']].merge(session_states, on='Bin', how='outer')\n",
    "    \n",
    "    # Align bins\n",
    "    multiplier = 1\n",
    "    event_type_list = ['goCueTrigger_times']  # , 'feedback_times', 'firstMovement_times'\n",
    "    event_type_name = ['Go cue']  # , 'Feedback time', 'First movement onset'\n",
    "    init = -1 * multiplier\n",
    "    end = 1.5 * multiplier\n",
    "    empirical_data = align_bin_design_matrix(init, end, event_type_list, session_trials, final_states, final_states['most_likely_states'], multiplier)\n",
    "    empirical_data = empirical_data.drop(columns=['new_bin'])\n",
    "    \n",
    "    \"\"\" Trial types \"\"\"\n",
    "    # Split in Foundtrial types\n",
    "    states_trial = states_per_trial_phase(empirical_data, session_trials, multiplier)\n",
    "    states_trial['mouse_name'] = mouse_name\n",
    "    states_trial['session'] = session\n",
    "    states_trial = broader_label(states_trial)\n",
    "    states_trial = states_trial[['Bin', 'Lick count', 'avg_wheel_vel', 'whisker_me', \n",
    "                                'l_paw_x', 'l_paw_y', 'r_paw_x', 'r_paw_y', 'most_likely_states',\n",
    "                                'correct', 'choice', 'contrast', 'block', 'reaction', 'response',\n",
    "                                'elongation', 'wsls', 'trial_id', 'goCueTrigger_times',\n",
    "                                'identifiable_states', 'label', 'mouse_name', 'session',\n",
    "                                'broader_label']]\n",
    "    \n",
    "    # Save to big df\n",
    "    states_trial_type = pd.concat([states_trial_type, states_trial], ignore_index=True)\n",
    "    del states_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_trial_type.to_parquet('states_trial_type_wheel_02-16-2026', compression='gzip')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial epoch barcoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_trial_type = pd.read_parquet('states_trial_type_wheel_02-16-2026')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_type_agg = ['correct_str', 'contrast_str', 'block_str', 'choice']\n",
    "\n",
    "plot = False\n",
    "target_length = 10\n",
    "\n",
    "all_sequences = pd.DataFrame(columns=['mouse_name', 'trial_type', 'broader_label', 'binned_sequence'])\n",
    "for m, mat in enumerate(sessions_to_process):\n",
    "\n",
    "    mouse_name = mat[0]\n",
    "    session = mat[1] \n",
    "    fit_id = str(mouse_name+session)\n",
    "\n",
    "    states_trial = states_trial_type.loc[states_trial_type['session']==session]\n",
    "    num_states = len(states_trial_type['most_likely_states'].unique())\n",
    "\n",
    "    \"\"\" Define trial types  \"\"\"\n",
    "    states_df = define_trial_types(states_trial, trial_type_agg)\n",
    "    vars = ['sample', 'trial_type', 'broader_label', 'mouse_name', 'most_likely_states']\n",
    "\n",
    "    df_grouped = states_df.groupby(vars[:-1])['most_likely_states'].apply(list).reset_index()\n",
    "    df_grouped.rename(columns={'most_likely_states': 'sequence'}, inplace=True)\n",
    "    df_grouped['binned_sequence'] = df_grouped['sequence'].apply(lambda seq: rescale_sequence(seq, target_length))  # New function removed NaNs\n",
    "    \n",
    "    if plot == True:\n",
    "        states_to_append = np.arange(0, num_states)\n",
    "        for i in range(100):\n",
    "            if df_grouped['broader_label'][i] == 'Quiescence':\n",
    "                plot_binned_sequence(df_grouped, i, states_to_append, 'viridis')\n",
    "    \n",
    "    all_sequences = pd.concat([all_sequences, df_grouped[['mouse_name', 'sample', 'trial_type', 'broader_label', 'binned_sequence']]], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences.to_parquet('all_sequences_02-16-2026', compression='gzip')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_sequence(seq, target_length, estimator):\n",
    "    \"\"\"\n",
    "    Rescales a categorical sequence to a fixed target length.\n",
    "    \n",
    "    - If `target_length` is smaller than the original length, it takes the mode of each bin.\n",
    "    - If `target_length` is larger, it repeats values evenly.\n",
    "    \n",
    "    Parameters:\n",
    "        seq (array-like): The original categorical sequence.\n",
    "        target_length (int): The desired length of the output sequence.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The transformed sequence with the specified target length.\n",
    "    \"\"\"\n",
    "    original_length = len(seq)\n",
    "\n",
    "    if original_length == target_length:\n",
    "        return np.array(seq)  # No change needed\n",
    "\n",
    "    if target_length < original_length:\n",
    "        # Compression: Split into bins and take mode of each bin\n",
    "        bins = np.array_split(seq, target_length)\n",
    "        # return np.array([mode(b)[0][0] for b in bins])  # Extract mode from result\n",
    "        if estimator == 'mode':\n",
    "            result = np.array([mode(b)[0] for b in bins])\n",
    "        elif estimator == 'mean':\n",
    "            result = np.array([np.mean(b) for b in bins])\n",
    "        return result  \n",
    "\n",
    "    else:\n",
    "        # Stretching: Repeat values to fit new size\n",
    "        stretched_indices = np.floor(np.linspace(0, original_length - 1, target_length)).astype(int)\n",
    "        return np.array(seq)[stretched_indices]  # Map stretched indices to original values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_binned_sequence(df_grouped, index, states_to_append, var_name, palette):\n",
    "        title = df_grouped['broader_label'][index]\n",
    "        fig, axs = plt.subplots(2, 1, sharex=False, sharey=True, figsize=(5, 2))\n",
    "        axs[0].imshow(np.concatenate([df_grouped[var_name+'_sequence'][index], states_to_append])[None,:],  \n",
    "                extent=(0, len(np.concatenate([df_grouped[var_name+'_sequence'][index], states_to_append])), \n",
    "                        0, 1),\n",
    "                aspect=\"auto\",\n",
    "                cmap=palette,\n",
    "                alpha=0.7) \n",
    "        axs[0].set_xlim([0, len(df_grouped[var_name+'_sequence'][index])])\n",
    "\n",
    "        axs[1].imshow(np.concatenate([df_grouped[var_name+'_binned_sequence'][index], states_to_append])[None,:],  \n",
    "                extent=(0, len(np.concatenate([df_grouped[var_name+'_binned_sequence'][index], states_to_append])), \n",
    "                        0, 1),\n",
    "                aspect=\"auto\",\n",
    "                cmap=palette,\n",
    "                alpha=0.7) \n",
    "        axs[1].set_xlim([0, len(df_grouped[var_name+'_binned_sequence'][index])])\n",
    "        axs[0].set_title(title)\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_type_agg = ['correct_str', 'contrast_str', 'block_str', 'choice']\n",
    "\n",
    "plot = False\n",
    "target_length = 10\n",
    "\n",
    "all_sequences = pd.DataFrame()\n",
    "for m, mat in enumerate(sessions_to_process):\n",
    "\n",
    "    mouse_name = mat[0]\n",
    "    session = mat[1] \n",
    "    fit_id = str(mouse_name+session)\n",
    "\n",
    "    states_trial = states_trial_type.loc[states_trial_type['session']==session]\n",
    "    num_states = len(states_trial_type['most_likely_states'].unique())\n",
    "\n",
    "    \"\"\" Define trial types  \"\"\"\n",
    "    states_df = define_trial_types(states_trial, trial_type_agg)\n",
    "    vars = ['sample', 'trial_type', 'broader_label', 'mouse_name', 'most_likely_states']\n",
    "    \n",
    "    \"\"\" Pre-process data  \"\"\"\n",
    "    states_df['whisker_me'] = zscore(np.array(states_df['whisker_me']), \n",
    "                                                    nan_policy='omit', axis=0)\n",
    "    states_df['l_paw_x_vel'] = states_df['l_paw_x'] * np.nan\n",
    "    states_df['l_paw_y_vel'] = states_df['l_paw_x'] * np.nan\n",
    "    states_df['r_paw_x_vel'] = states_df['l_paw_x'] * np.nan\n",
    "    states_df['r_paw_y_vel'] = states_df['l_paw_x'] * np.nan\n",
    "    \n",
    "    states_df['l_paw_x_vel'][1:] =zscore(np.diff(states_df['l_paw_x']/2), \n",
    "                                                    nan_policy='omit', axis=0)\n",
    "    states_df['l_paw_y_vel'][1:] = zscore(np.diff(states_df['l_paw_y']/2), \n",
    "                                                    nan_policy='omit', axis=0)\n",
    "    states_df['r_paw_x_vel'][1:] = zscore(np.diff(states_df['r_paw_x']), \n",
    "                                                    nan_policy='omit', axis=0)\n",
    "    states_df['r_paw_y_vel'][1:] = zscore(np.diff(states_df['r_paw_y']), \n",
    "                                                    nan_policy='omit', axis=0)\n",
    "\n",
    "    process_vars = ['Lick count', 'whisker_me', 'l_paw_x_vel', 'l_paw_y_vel', 'r_paw_x_vel', 'r_paw_y_vel']\n",
    "    all_vars_grouped = pd.DataFrame()\n",
    "    \n",
    "    for v, variable in enumerate(process_vars):\n",
    "        \n",
    "        df_grouped = states_df.groupby(vars[:-1])[variable].apply(list).reset_index()\n",
    "        df_grouped.rename(columns={variable: variable+'_sequence'}, inplace=True)\n",
    "        if v == 0:\n",
    "            df_grouped[variable+'_binned_sequence'] = df_grouped[variable+'_sequence'].apply(lambda seq: rescale_sequence(seq, target_length, 'mode'))  # New function removed NaNs\n",
    "            all_vars_grouped = df_grouped.copy()\n",
    "        else:\n",
    "            df_grouped[variable+'_binned_sequence'] = df_grouped[variable+'_sequence'].apply(lambda seq: rescale_sequence(seq, target_length, 'mean'))  # New function removed NaNs\n",
    "            all_vars_grouped = all_vars_grouped.merge(df_grouped, on=['sample', 'trial_type', 'broader_label', 'mouse_name'])\n",
    "        all_vars_grouped = all_vars_grouped.drop(columns=[variable+'_sequence'])\n",
    "            \n",
    "        # if plot == True:\n",
    "        #     states_to_append = np.arange(0, num_states)\n",
    "        #     for i in range(10):\n",
    "        #         if df_grouped['broader_label'][i] == 'Choice':\n",
    "        #             plot_binned_sequence(df_grouped, i, states_to_append, variable, 'viridis')\n",
    "    \n",
    "    all_sequences = pd.concat([all_sequences, all_vars_grouped], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wheel syllable binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_type_agg = ['correct_str', 'contrast_str', 'block_str', 'choice']\n",
    "\n",
    "plot = False\n",
    "target_length = 10\n",
    "\n",
    "all_sequences = pd.DataFrame()\n",
    "for m, mat in enumerate(sessions_to_process):\n",
    "\n",
    "    mouse_name = mat[0]\n",
    "    session = mat[1] \n",
    "    fit_id = str(mouse_name+session)\n",
    "\n",
    "    states_trial = states_trial_type.loc[states_trial_type['session']==session]\n",
    "    num_states = len(states_trial_type['most_likely_states'].unique())\n",
    "\n",
    "    \"\"\" Define trial types  \"\"\"\n",
    "    states_df = define_trial_types(states_trial, trial_type_agg)\n",
    "    vars = ['sample', 'trial_type', 'broader_label', 'mouse_name', 'most_likely_states']\n",
    "    \n",
    "    \"\"\" Pre-process data  \"\"\"\n",
    "    states_df['avg_wheel_vel'] = np.array(np.abs(states_df['avg_wheel_vel']))\n",
    "    # states_df['l_paw_x_vel'] = states_df['l_paw_x'] * np.nan\n",
    "    # states_df['l_paw_y_vel'] = states_df['l_paw_x'] * np.nan\n",
    "    # states_df['r_paw_x_vel'] = states_df['l_paw_x'] * np.nan\n",
    "    # states_df['r_paw_y_vel'] = states_df['l_paw_x'] * np.nan\n",
    "    \n",
    "    # states_df['l_paw_x_vel'][1:] =zscore(np.diff(states_df['l_paw_x']/2), \n",
    "    #                                                 nan_policy='omit', axis=0)\n",
    "    # states_df['l_paw_y_vel'][1:] = zscore(np.diff(states_df['l_paw_y']/2), \n",
    "    #                                                 nan_policy='omit', axis=0)\n",
    "    # states_df['r_paw_x_vel'][1:] = zscore(np.diff(states_df['r_paw_x']), \n",
    "    #                                                 nan_policy='omit', axis=0)\n",
    "    # states_df['r_paw_y_vel'][1:] = zscore(np.diff(states_df['r_paw_y']), \n",
    "    #                                                 nan_policy='omit', axis=0)\n",
    "\n",
    "    process_vars = ['avg_wheel_vel']\n",
    "    all_vars_grouped = pd.DataFrame()\n",
    "    \n",
    "    for v, variable in enumerate(process_vars):\n",
    "        \n",
    "        df_grouped = states_df.groupby(vars[:-1])[variable].apply(list).reset_index()\n",
    "        df_grouped.rename(columns={variable: variable+'_sequence'}, inplace=True)\n",
    "        if v == 0:\n",
    "            df_grouped[variable+'_binned_sequence'] = df_grouped[variable+'_sequence'].apply(lambda seq: rescale_sequence(seq, target_length, 'mode'))  # New function removed NaNs\n",
    "            all_vars_grouped = df_grouped.copy()\n",
    "        else:\n",
    "            df_grouped[variable+'_binned_sequence'] = df_grouped[variable+'_sequence'].apply(lambda seq: rescale_sequence(seq, target_length, 'mean'))  # New function removed NaNs\n",
    "            all_vars_grouped = all_vars_grouped.merge(df_grouped, on=['sample', 'trial_type', 'broader_label', 'mouse_name'])\n",
    "        all_vars_grouped = all_vars_grouped.drop(columns=[variable+'_sequence'])\n",
    "            \n",
    "        # if plot == True:\n",
    "        #     states_to_append = np.arange(0, num_states)\n",
    "        #     for i in range(10):\n",
    "        #         if df_grouped['broader_label'][i] == 'Choice':\n",
    "        #             plot_binned_sequence(df_grouped, i, states_to_append, variable, 'viridis')\n",
    "    \n",
    "    all_sequences = pd.concat([all_sequences, all_vars_grouped], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences.to_parquet('all_sequences_wheel_10-07-2025', compression='gzip')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iblenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

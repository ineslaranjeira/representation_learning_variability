{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RlSlPEBmJonU"},"outputs":[],"source":["%matplotlib notebook\n","\n","import os\n","import pickle\n","import copy\n","\n","import autograd.numpy as np\n","import autograd.numpy.random as npr\n","npr.seed(12345)\n","\n","import matplotlib.pyplot as plt\n","from matplotlib import gridspec\n","from sklearn.decomposition import PCA\n","from scipy import linalg\n","\n","import seaborn as sns\n","color_names = [\"windows blue\", \"red\", \"amber\", \"faded green\"]\n","colors = sns.xkcd_palette(color_names)\n","sns.set_style(\"white\")\n","sns.set_context(\"talk\")\n","\n","import ssm"]},{"cell_type":"markdown","metadata":{"id":"HjnPBBrNJonV"},"source":["# Interpreting nonlinear dynamical systems using approximate models\n","\n","In lecture today, we saw how fitting a recurrent switching linear dynamical system (rSLDS) to neural recording data let us interpret that data using methods from linear dynamical systems.\n","\n","This is very convenient because we know a lot of ways to study linear dynamical systems, whereas nonlinear systems are a huge mess.\n","\n","## üìñ Contents of this notebook\n","\n","üèÉ = coding exercises  \n","üê∏ = extra things to experiment with if you have time  \n","ü§î = thought exercises\n","\n","1. üåÄ Autonomous dynamical systems ($\\dot{x} = f(\\mathbf{A}x)$)  \n","    1.1 Our friend the Poincar√© diagram  \n","    1.2 The `simulators.autonomous` class for simulating autonomous systems  \n","    1.3 Case study: the Lorenz system  \n","        1.3.1 Simulating dynamics  \n","        1.3.2 üèÉ Visualizing the flow field  \n","        1.3.2 üèÉ Fitting rSLDS\n","        1.3.3 üèÉ Plotting the ELBO\n","        1.3.4 üèÉ Visualizing the fit flow field\n","        1.3.5 üèÉ Choosing the number of states  \n","2. üèéÔ∏è Input-driven dynamical systems ($\\dot{x} = f(\\mathbf{A}x + \\mathbf{V}u$))  \n","    2.1 Simulating a leaky integrator RNN\n","        2.1.1 Getting to know the dynamics matrix\n","        2.1.2 üèÉ Picking the size of the fit dynamical system\n","        2.1.3 üèÉ Fitting rSLDS with inputs\n","        2.1.4 üèÉ Plotting the ELBO and visualizing the fit flow field\n","        2.1.5 üèÉ Getting the inputs wrong\n","        2.1.6 üèÉ Examining parameters of the fit model\n","    2.2 üê∏ further RNN adventures\n","\n","**Acknowledgement:** parts of this notebook are adapted from Scott Linderman's Recurrent SLDS notebook, which can be found <a href=https://github.com/lindermanlab/ssm/blob/master/notebooks/4-Recurrent-SLDS.ipynb>here</a>!"]},{"cell_type":"markdown","metadata":{"id":"naOSN0niJonR"},"source":["# SETUP\n","\n","Go to [the SSM Github repository](https://github.com/lindermanlab/ssm) and follow the **Installation** instructions:\n","```\n","git clone https://github.com/lindermanlab/ssm\n","cd ssm\n","pip install numpy cython\n","pip install -e .\n","```\n","Then, download [simulators.py](https://drive.google.com/file/d/1o5zAZ5sKi21kvEDbMGqPzs0QgVnpNPzz/view?usp=sharing) which contains a set of helper functions that simulate the dynamics of dynamical systems. Upload it to your google drive and stick it inside `/content/ssm` (which we created above.)"]},{"cell_type":"markdown","metadata":{"id":"0Bqyw6xqJonW"},"source":["# 1. üåÄ Autonomous dynamical systems ($\\dot{x} = f(\\mathbf{A}x)$)  \n","A dynamical system is called autonomous (or homogeneous) if $\\dot{x}=0$ when $x=0$, meaning we're studying its dynamics in the absence of any ongoing external input. Broadly speaking, nonlinear autonomous systems can do one of three things:  \n","<ol>\n","    <li> all terms either explode to infinity or decay to 0 (though they can do interesting things along the way)  \n","    <li> produce oscillations (<a href=https://www.youtube.com/watch?v=jRQAndvF4sM>here's</a> a chemical system doing this)\n","    <li> produce chaos (first demonstrated in neural net models <a href=https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.61.259>here</a>, a more approachable read <a href=https://journals.aps.org/pre/abstract/10.1103/PhysRevE.82.011903>here</a>.)\n","</ol>\n","    \n","\n","## 1.1 Using the Poincar√© diagram to predict the behavior of a linear autonomous system\n","In a **linear** autonomous system of the form $\\dot{x} = \\mathbf{A}x$, we are restricted to case 1 above: exploding to +/-infinity or decaying to zero (or hovering at a fixed value, but this requires very precise tuning of the values of $\\mathbf{A}$.)\n","\n","Because the behavior of the system is entirely determined by its dynamics matrix $\\mathbf{A}$, we can look at features of $\\mathbf{A}$ to understand just what the system will do:\n","<ol>\n","    <li>The <a href=https://en.wikipedia.org/wiki/Determinant>Determinant</a> of $\\mathbf{A}$, which  is the product of its eigenvalues: $\\textrm{det}(\\mathbf{A}) = \\prod_{i=1}^n \\lambda_i$\n","    <li> The <a href=https://en.wikipedia.org/wiki/Trace_(linear_algebra)>Trace</a> of $\\mathbf{A}$, which is the sum of its eigenvalues: $\\textrm{tr}(\\mathbf{A}) = \\sum_{i=1}^n \\lambda_i$\n","</ol>\n","\n","Given these two values, the Poincar√© diagram describes the expected behavior of the system:\n","<div align=center>\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/Stability_Diagram.png/825px-Stability_Diagram.png\" width=600>\n","    Fig: Back to haunt you from undergraduate math, the Poincare diagram captures the space of possible behaviors of linear dynamical systems, given the trace and determinant of their dynamics matrix A.\n","</div>\n","\n","This also means that if we want to hand-craft a system to behave a certain way, we just have to mess with its eigenvalues."]},{"cell_type":"markdown","metadata":{"id":"Z-ZEzfrbJonX"},"source":["## üñ•Ô∏è 1.2 `simulators.autonomous` : A Python class for simulating dynamical systems\n","We'll use the provided python module `simulators.py` to simulate the dynamics of some autonomous and driven dynamical systems. This class has a few helpful functions:\n","\n","#### `autonomous.set_state(args)` sets the state of the system\n","> Inputs:\n","> * `args` is a tuple specifying the value for each state variable in your system. Make sure it's the right size.  \n",">\n","> Usage: `lorenz.set_state((x,y,z))` to set the state to $x,y,z$ (see next section).  \n","\n","#### `autonomous.set_timestep(dt)` sets the simulation timestep\n","> Inputs:\n","> * `dt` is a simulation timestep in somewhat arbitrary units.\n","> This defaults to 0.01, you probably shouldn't have to change it.  \n","\n","#### `autonomous.dynamics(time, (X))` computes the instantaneous dynamics of the system\n","> Inputs:  \n","> * `time` allows for the system equations to include an explicit dependence on time (not the case for Lorenz.)  \n","> * `X` optionally allows you to specify a system state, otherwise the current state is used.  \n",">\n","> Returns:\n","> * `args` the derivative of the system, i.e. $(\\dot{x},\\dot{y},\\dot{z})$ for Lorenz.  \n","\n","#### `autonomous.simulate(tmax, (persistent))` simulates the system dynamics for `time` seconds\n","> Inputs:  \n","> * `persistent` (defaults to `False`) will cause the system state to remain at the value at the end of the simulation, otherwise the system state is unchanged (so two runs of the system will produce identical trajectories.)  \n",">\n","> Returns:\n","> * `args`: the state variables of the system on all simulation timesteps (default timestep is `model.dt = 0.01`)\n","\n","#### `autonomous.plot_simulation(tmax, (axes, pca))` plots a simulation of the system\n","> Inputs:\n","> * `tmax` is how long to simulate.\n","> * `axes` lets you pass a set of `matplotlib` axes on which to plot the system trajectory. Otherwise makes a new figure.\n","> * `pca` (defaults to `False`) instructs the model to plot the first 3 PCs of the system when True, otherwise only the first 3 dimensions are plotted.\n","\n","#### `autonomous.plot_timeseries(tmax, (axes, normalize))` plots each system dimension as a function of time\n","> Same arguments as `plot_simulation` with the exception of:\n","> * `normalize` (defaults to `True`) scales each plotted dimension to be bounded on the interval $[0,1]$\n","\n","#### `autonomous.animate_simulation(tmax, (axes, pca, step))` makes an animated version of `plot_simulation`\n","> Same arguments as `plot_simulation` with the exception of:\n","> * `step` (defaults to 50) is the animation time step in units of $dt$. If it's too small you'll run out of memory.\n","\n","#### `autonomous.observe_simulation(n_obs, (sigma, transform))` makes a noisy `n_obs`-dimensional observation of the system simulation\n","> We'll use this to experiment with how rSLDS (or other models) performs when given noisy observations of the underlying dynamical system it is fitted to.\n","> Inputs:\n","> * `n_obs` is the number of observations to make. Each observation is related to the system variables as $y_n = \\sum_{i=1}^D w_i x_i$, where $w_i \\sim \\mathcal{N}(0,1/\\sqrt{D})$ and $D$ is the number of system variables.\n","> * `sigma` (defaults to 0) is the variance of an added Gaussian noise term. Default is no noise added.\n","> * `transform` (defaults to 'linear') lets you apply a nonlinear transform to your observations. Options are `linear`, `nonneg`, and `tanh`."]},{"cell_type":"markdown","metadata":{"id":"gBYznP0dJonX"},"source":["## 1.3 Autonomous system case study: the Lorenz Attractor\n","\n","The Lorenz attractor (not <a href=https://amenteemaravilhosa.com.br/wp-content/uploads/2018/04/konrad-lorenz-1.jpg>that Lorenz</a>) is a well-known autonomous nonlinear dynamical system that produces chaotic dynamics for certain parameter settings. It is characterized by a system of three equations $\\dot{x},\\dot{y},\\dot{z}$ with three fixed parameters $\\sigma, \\rho, \\beta$:\n","\n","\\begin{align}\n","\\dot{x} & = (y-x)\\sigma \\\\\n","\\dot{y} & = \\rho x - y - xz \\\\\n","\\dot{z} & = -\\beta z + xy\n","\\end{align}\n","\n","By default the model sets $\\sigma=10, \\beta=2.667,$ and $rho=28$, which produces the iconic chaotic butterfly. Let's take a look using some of the built-in functions of the `simulator` class:"]},{"cell_type":"markdown","metadata":{"id":"U1DaE1uEJonY"},"source":["### 1.3.1 Simulating the dynamics of the Lorenz system\n","For this section I've given you the required code, just to make sure everyone's on the right page:"]},{"cell_type":"code","source":["os.chdir('/content/ssm')\n","!ls\n","import numpy as np"],"metadata":{"id":"7O87GSGiU3xw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"C7H-SK8iU3iH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kg8odbPcJonY"},"outputs":[],"source":["%matplotlib notebook\n","import simulators\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","lorenz = simulators.lorenz()\n","\n","# Plot the Lorenz attractor using a Matplotlib 3D projection.\n","fig = plt.figure(figsize=(10,5))\n","ax = fig.add_subplot(121, projection='3d', aspect='auto')\n","\n","lorenz.plot_simulation(tmax=100, ax=ax, pca=False)\n","\n","ax2 = fig.add_subplot(122)\n","lorenz.plot_timeseries(ax=ax2, normalize=True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"OpAHz0-0JonZ"},"source":["And just for fun, we can animate these dynamics as well, to get a visual impression of how they depend on the system state. Note, this animation projects the dynamics onto their first two principal components:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wctTIXqhJonZ"},"outputs":[],"source":["from matplotlib import rc\n","rc('animation', html='jshtml')\n","\n","fig = plt.figure(figsize=(5,5))\n","ax = fig.add_subplot(111, aspect='auto')\n","plt.xlim(-30, 30)\n","plt.ylim(-30, 30)\n","\n","lorenz.simulate(10, persistent=True);  # setting persistent=True \"burns-in\" to get the system into its steady-state behavior\n","anim = lorenz.animate_simulation(tmax=20, ax=ax, step=50); # and animation! this takes a bit of time to process\n","anim"]},{"cell_type":"markdown","metadata":{"id":"Cv7TAR5VJonZ"},"source":["### 1.3.2 üèÉ Visualizing the true flow field of the Lorenz system\n","Before we go fitting any fancy models to these dynamics, we should see what the true underlying flow field of the system looks like.\n","\n","Complete the code section below to generate an approximate 2D flow field of the system when we project it onto its first two PCs. Some functions that will help you along the way:\n","* `sklearn.decomposition.PCA` it's, uh, PCA\n","* `lorenz.step((x,y,z))` will return the derivative of the model at the point $(x,y,z)$. But how do we get $(x,y,z)$ from a point\n","* `ax.quiver` to plot a flow-field"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrRqLRL5JonZ"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# setting up a grid of points at which to compute the flow field:\n","xlim = [-35, 35]\n","ylim = [-30, 30]\n","xx = np.linspace(*xlim, 35)\n","yy = np.linspace(*ylim, 30)\n","Xpts, Ypts = np.meshgrid(xx, yy)\n","xy = np.column_stack((Xpts.ravel(), Ypts.ravel()))\n","\n","# sample the dynamics of the system to find its PCs\n","\n","\n","# compute the projection of the flow onto the first 2 PCs\n"]},{"cell_type":"markdown","metadata":{"id":"6YnlziBjJona"},"source":["üê∏ Of course, we didn't have to project onto the first 2 PCs of our data to make the flow field. What other kinds of 2D flow field can you generate?  \n","üê∏ It turns out that `ax.quiver` does also allow you to plot a 3D flow field. These are usually too messy to get much visual intuition from- can you figure out any tricks to visualize the full 3D flow field of the Lorenz system?\n","\n","\n","### ü§î Looking at your flow field- how many linear systems would you use to approximate it? What would their dynamics do?\n","I bolded this ü§î so you don't miss it, because up next we're going to find out what rSLDS does."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VEuBREsjJona"},"outputs":[],"source":["ncells = 50\n","raster = lorenz.observe_simulation(ncells, tmax=100, sigma=0)\n","\n","plt.figure(figsize=(8,6))\n","plt.imshow(raster.T,aspect='auto')\n","plt.title('Some noisy observations of a Lorenz system')\n","plt.colorbar();"]},{"cell_type":"markdown","metadata":{"id":"Y-CWDpWpJona"},"source":["### 1.3.3 Fitting Recurrent SLDS to the Lorenz system\n","First, a refresher on the rSLDS from the original notebook by Scott Linderman. This section reproduces some text from the [original rSLDS tutorial notebook](https://github.com/lindermanlab/ssm/blob/master/notebooks/4-Recurrent-SLDS.ipynb) for some background:\n","\n","> A Recurrent Switching Linear Dynamical System (rSLDS) is a generalization of a Switching Linear Dynamical System (SLDS) in which the switches in the discrete state are allowed to depend on the value of the continuous state (hence the name recurrent). The rSLDS was developed by Linderman _et al_ in [\"Bayesian Learning and Inference in Recurrent Switching Linear Dynamical Systems\"](http://proceedings.mlr.press/v54/linderman17a.html).\n",">\n",">### Generative model for rSLDS\n","The generative model for rSLDS is the same as the SLDS case, except that the discrete state transition probabilities are modulated by the continuous state.\n",">\n",">1. **Discrete State Update**. At each time step, sample a new discrete state $z_t \\mid z_{t-1}, x_{t-1}$ with probabilities driven by a logistic regression on the continuous state:\n","$$\n","p(z_t = i \\mid z_{t-1} = j, x_{t-1}) \\propto\n","\\exp{\\left( \\log (P_{j,i}) + W_i^T u_t + R_i ^T x_{t-1} \\right)}\n","$$\n","where $W_i$ is a vector of weights associated with discrete state $i$, that control dependence on an external, known input $u_t$. $R_i$ is again a vector of weights assocaited with state $i$, which weights the contribution from the prior state.\n",">\n",">\n","> 2. **Continuous State Update**. Update the state using the dynamics matrix corresponding to the new discrete state:\n","$$\n","x_t = A_k x_{t-1} + V_k u_{t} + b_k + w_t\n","$$\n","$A_k$ is the dynamics matrix corresponding to discrete state $k$. $u_t$ is the input vector (specified by the user, not inferred by SSM) and $V_k$ is the corresponding control matrix. The vector $b$ is an offset vector, which can drive the dynamics in a particular direction.\n","The terms $w_t$ is a noise terms, which perturbs the dynamics.\n","Most commonly these are modeled as zero-mean multivariate Gaussians,\n","but one nice feature of SSM is that it supports many distributions for these noise terms. See the Linear Dynamical Systems notebook for a list of supported dynamics models.\n",">\n",">\n","> 3. **Emission**. We now make an observation of the state, according to the specified observation model. In the general case, the state controls the observation via a Generalized Linear Model:\n","$$\n","y_t \\sim \\mathcal{P}(\\eta(C_k x_t + d_k + F_k u_t + v_t))\n","$$\n","$\\mathcal{P}$ is a probabibility distribution. The inner arguments form an affine measurement of the state, which is then passed through the inverse link function $\\eta(\\cdot)$.\n","In this case, $C_k$ is the measurement matrix corresponding to discrete state $k$, $d_k$ is an offset or bias term corresponding to discrete state $k$, $F_k$ is called the feedthrough matrix or passthrough matrix (it passes the input directly to the emission). In the Gaussian case, the emission can simply be written as $y_t = C_k x_t + d_k + F_k u_t + v_t$ where $v_t$ is a Gaussian r.v. See the Linear Dynamical System notebook for a list of the observation models supported by SSM.\n","  \n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"U_7pqx8bJona"},"source":["### Types of transition models\n","The most general form of the discrete state transition model is the following:\n","$$\n","p(z_t = i \\mid z_{t-1} = j, x_{t-1}) \\propto\n","\\exp{\\left( \\log (P_{j,i}) + w_i^T u_t + r_i ^T x_{t-1} \\right)}\n","$$\n","\n","(we write $\\propto$ and not $=$ just because we normalize these probabilities to sum to 1.) The term in the exponent has three parts:\n","1. $\\log (P_{j,i})$ is a transition probability term, akin to the transition matrix of a Hidden Markov model.\n","2. $w_i^T u_t$ is an external input term- our Lorenz system is autonomous, so this will just be zero for us.\n","3. $ r_i ^T x_{t-1}$ is the \"recurrent\" part of the model- it allows the current latent state of the system $x$ to influence the probability of changing to a different discrete state.\n","\n","When fitting any SLDS model, SSM provides several kinds of transition models. These are implemented as classes in `ssm/transitions.py`. The model `transitions` parameter can be set to the following strings:\n","* `standard` the typical HMM transition matrix (ie only term 1 above).\n","* `sticky` as above, but with a prior to bias the model towards self-transitions (ü§î why would that be useful?)\n","* `input_driven` implements the transition matrix as a GLM applied to the external input term (modified version of term 2 above).\n","* `recurrent` includes terms 1 and 3 above (and term 2 if there is input present.)\n","* `recurrent_only` replaces term 1 above with a constant \"bias\" term, so that transition probabilities are only dependent on the state of the system $x$.\n","\n","One more ü§î from Scott's tutorial:\n",">What happens as the magnitude of the entries in $R_i$ become very large (compared to the entries of $R_j$ for the other states? Do the transitions become more or less random?  \n","\n","### Types of dynamics models\n","The latent dynamics take the form of an autoregressive process:\n","$$\n","x_t = A_k x_{t-1} + V_k u_{t} + b_k + w_t\n","$$\n","This again is the most general form of the model, and other, simpler forms are available. SSM provides the following dynamics model options, which are implemented in `ssm/observations.py` and specified by setting the model `dynamics` parameter. These mostly vary in their noise assumptions, and are less critical to play with:\n","* `gaussian` assumes the latent state dynamics evolve as above, and treats Gaussian observation noise with a full positive definite covariance matrix. This is usually avoided because it's a lot of paramaeters to fit for little gain.\n","* `diagonal_gaussian` is as above, but observation noise is assumed to be independent between latent variables (ie covariance matrix is constrained to be diagonal.)\n","\n","\n","### Types of emissions models\n","The emissions model defines how the latent dynamics are translated to observed (e.g., neural) activity. Most broadly, it takes the form:\n","$$\n","y_t \\sim \\mathcal{P}(\\eta(C_k x_t + d_k + F_k u_t + v_t))\n","$$\n","SSM provides may forms of the emissions model, which are fond in `ssm/emissions.py` and specified by setting the model `emissions` parameter. These all extend a set of linear emissions base models:\n","* **Standard**: $E[y | x] = Cx + d + Fu$ is a simple linear mapping from x and u to y, with no constraints on model parameters.\n","* **Orthogonal**: the emissions matrix $C$ is constrained to be orthogonal.\n","* **Identity**: the emissions matrix $C$ is the identity matrix, so the observed variables are just noisy versions of the latents.\n","\n","Each of these three flavors can be applied to various nonlinear emissions model. To do so, change the model specification string `XXX` to `XXX_orthog` or `XXX_id` to use the orthogonality or identity constraints, respectively (eg `gaussian_orthog` and `gaussian_id`.) Here are some emissions models you could try:\n","* `gaussian` is the standard observation model, assuming Gaussian observation noise.\n","* `studentst` is the Student's T distribution, which allows heavier tails than the standard Gaussian.\n","* `poisson` is... yes, Poisson. Good for spiking data, perhaps?\n","* `bernoulli` for Bernoulli variables, again potentially useful for spiking."]},{"cell_type":"markdown","metadata":{"id":"wv5THIgYJona"},"source":["### The actual fitting part\n","Now, let's create a new rSLDS object and fit it to the data sampled from our Lorenz system. Note that we're only giving the system access to the output of `observe_simulation`, it doesn't get to see the true Lorenz system variables.\n","\n","We'll be using the `\"laplace_em\"` fitting method, which requires that we use `\"structured_meanfield\"` for the `variational_posterior` argument. There's more information on this in section 4.1 of the <a href=https://github.com/lindermanlab/ssm/blob/master/notebooks/4-Recurrent-SLDS.ipynb>rSLDS notebook</a> but the tldr is that of two implemented fitting methods for SLDS, Laplace Variational EM (`\"laplace_em\"`) consistently converges faster.\n","\n","The fitting process involves three steps: creating the model, initializing it to sensible values, and then running fitting. We do these with the following functions:\n","\n","#### `ssm.SLDS(D_obs, K, D_latent, (transitions, dynamics, emissions, single_subspace)` creates the model object\n","> Inputs:\n","> * `D_obs` is the number of observed dimensions.\n","> * `K` is the number of fit discrete states.\n","> * `D_latent` is the number of fit latent dimensions.\n","> * `transitions` specifies the transition model (see above). Recommend `recurrent_only`.\n","> * `dynamics` specifies the dynamics model (see above). Recommend `diagonal_gaussian`.\n","> * `emissions` specifies-- you guessed it!-- the emissions model (see above). Recommend `gaussian_orthog`.\n",">\n","> Outputs:\n","> * an ssm model object of type SLDS (rSLDS is a subtype of the SLDS class.)\n","\n","#### `model.initialize(data)` initializes model parameters to sensible values\n","> This is a surprisingly important step! Discrete models often get stuck in local minima, so when we fit the rSLDS we're actually going to start off by fitting an autoregressive Hidden Markov Model to get our estimates of the `transitions` and `dynamics` matrices. Take a look at `ssm/lds.py` to see what happens here.\n","\n","#### `model.fit(data, (params))` fits the model, as you might expect\n","> Inputs:\n","> * `data` is your observed system state.\n","> * `method` is the fitting algorithm, keep this to `\"laplace_em\"`.\n","> * `variational_posterior` is the method for approximating the posterior of the data, keep this to `\"structured_meanfield\"`.\n","> * `num_iters` specifies how long to run fitting.  \n",">\n","> Outputs:\n","> * `elbos` is the Evidence Lower Bound (ELBO) on the log-likelihood of the data at each iteration of fitting, to check convergence.\n","> * `posterior` is a list of the posterior estimates of latent variables $x(t)$ and discrete states $z(t)$ given the data $y(t)$ used for fitting. Each list entry is one trial- so if you're fitting to a single recording just take the index `[0]`.\n"]},{"cell_type":"markdown","metadata":{"id":"leY3h8Q5Jonb"},"source":["### üèÉ Time to fit\n","Now, let's generate some observation data from our Lorenz system, and fit our model! Here we can check the quality of the observation data quickly:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSG9l7-NJonb"},"outputs":[],"source":["ncells = 20  # however many you like\n","lorenz = simulators.lorenz()  # start with a clean slate\n","raster = lorenz.observe_simulation(ncells, tmax=100, sigma=3)  # simulate your model, potentially with noisy observations\n","\n","pca_data   = PCA(n_components=2)\n","pcs_data   = pca_data.fit_transform(raster)\n","\n","fig = plt.figure(figsize=(5,5))\n","ax = fig.add_subplot(111)\n","plt.plot(pcs_data[:,0], pcs_data[:,1], lw=1)\n","plt.title('first 2 PCs of data')"]},{"cell_type":"markdown","metadata":{"id":"pDrns_NZJonb"},"source":["Now complete the code section below using the three functions above, making sure to save the `elbos` and `posteriors` outputted by the fitting process.\n","\n","The `model.fit` step takes 4 to 5 seconds per iteration on a laptop (using ncells=20 and tmax=100), and often converges within 50 iterations- so let's start there."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"gqeoMaN6Jonb"},"outputs":[],"source":["# Global parameters you'll need to set:\n","D_obs = _                # number of observed dimensions\n","D_latent = _             # number of latent dimensions - up to you!\n","K = _                    # number of latent states to fit - up to you!\n","\n","# Now fit that model:\n"]},{"cell_type":"markdown","metadata":{"id":"gA1ADwAtJonb"},"source":["### üèÉ 1.3.3 Checking Convergence\n","Okay that was fun- but what just happened? To see if things worked, you should first take a look at the [evidence lower bound (ELBO)](https://en.wikipedia.org/wiki/Evidence_lower_bound) to confirm that the fitting process has converged.\n","\n","Write some code to do so below. **Note**: the ELBO on the very first fitting step is usually awful, so you should drop it and just plot from the second step onward."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7fvi2AuYJonb"},"outputs":[],"source":["# plot the ELBO to check for convergence"]},{"cell_type":"markdown","metadata":{"id":"8m6P6GX_Jonb"},"source":["### 1.3.4 Visualizing the fit model\n","Assuming the model converged, we should take a look at the fit it produced. You've had to figure out how to plot a fit flow field once already, so for this part you can just use these helper functions.\n","\n","(One note, the size of the flow-field arrows can be compared within a state, but due to `quiver`'s built-in scaling you can't actually compare them between states.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1G7sxYzAJonc"},"outputs":[],"source":["def plot_trajectory(z, x, ax=None, ls=\"-\"):\n","    zcps = np.concatenate(([0], np.where(np.diff(z))[0] + 1, [z.size]))\n","    if ax is None:\n","        fig = plt.figure(figsize=(4, 4))\n","        if len(x.shape)==3:\n","            ax = fig.gca(projection='3d')\n","        else:\n","            ax = fig.gca()\n","    for start, stop in zip(zcps[:-1], zcps[1:]):\n","        if len(x.shape)==3:\n","            ax.plot(x[start:stop + 1, 0],\n","                    x[start:stop + 1, 1],\n","                    x[start:stop + 1, 2],\n","                    lw=1, ls=ls,\n","                    color=colors[z[start] % len(colors)],\n","                    alpha=0.4)\n","        else:\n","            ax.plot(x[start:stop + 1, 0],\n","                    x[start:stop + 1, 1],\n","                    lw=1, ls=ls,\n","                    color=colors[z[start] % len(colors)],\n","                    alpha=0.4)\n","    return ax\n","\n","\n","def plot_flattened_dynamics(model, pca, trajectory, xlim=None, ylim=None, nxpts=20, nypts=20, alpha=0.8, ax=None, figsize=(3, 3)):\n","\n","    PCs   = pca.transform(trajectory)\n","    if xlim is None:\n","        xlim = (min(PCs[:,0])-1, max(PCs[:,0])+1)\n","        ylim = (min(PCs[:,1])-1, max(PCs[:,1])+1)\n","\n","    x = np.linspace(*xlim, nxpts)\n","    y = np.linspace(*ylim, nypts)\n","    X, Y = np.meshgrid(x, y)\n","    xy = np.column_stack((X.ravel(), Y.ravel()))\n","\n","    fullD = pca.inverse_transform(xy)\n","\n","    # Get the probability of each state at each xy location\n","    # Note that this assumes the `recurrent_only` form of the transition model-\n","    # if you change that you may have to improvise.\n","    z = np.argmax(fullD.dot(model.transitions.Rs.T) + model.transitions.r, axis=1)\n","\n","    if ax is None:\n","        fig = plt.figure(figsize=[7,7])\n","        ax = fig.add_subplot(111)\n","    plt.plot(PCs[:,0], PCs[:,1], lw=0.75, color='k', alpha=0.4)\n","\n","    # project our D-dimensional dynamics vector onto the first 2 PCs\n","    for k, (A, b) in enumerate(zip(model.dynamics.As, model.dynamics.bs)):\n","        dxydt_m = pca.transform((fullD.dot(A.T) + b - fullD))\n","\n","        zk = z == k\n","        if zk.sum(0) > 0:\n","            ax.quiver(xy[zk, 0], xy[zk, 1], dxydt_m[zk, 0], dxydt_m[zk, 1], color=colors[k % len(colors)], alpha=1.0)\n","\n","    plt.tight_layout()\n","\n","    return ax"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OmjObeviJonc"},"outputs":[],"source":["# TODO: get the values of xhat and zhat\n","xhat = _\n","zhat = _\n","\n","# everything else here you can keep as is:\n","\n","# get the first 2 PC's of the \"neural\" data\n","pca_data   = PCA(n_components=2)\n","pcs_data   = pca_data.fit_transform(raster)\n","\n","# get the first 2 PC's of the latent dimensions of the fit model\n","pca_model  = PCA(n_components=2)\n","pcs_model  = pca_model.fit_transform(xhat)\n","\n","\n","# and plot everything!\n","fig = plt.figure(figsize=(8,8))\n","\n","# plot the original system\n","ax = fig.add_subplot(221)\n","plt.plot(pcs_data[:,0], pcs_data[:,1], lw=1)\n","plt.title('first 2 PCs of data')\n","\n","# plot the inferred latent trajectories and states\n","ax2 = fig.add_subplot(222)\n","plot_trajectory(zhat, pcs_model, ax=ax2)\n","plt.title('most-likely state of fit latents')\n","\n","# add some plots here to compare the PCs of the observed data to those of the fit system:\n","ax2 = fig.add_subplot(212)\n","plt.plot(pcs_data[0])\n","plt.plot(pcs_model[0])\n","\n","\n","fig2 = plt.figure(figsize=(8,8))\n","ax = fig2.add_subplot(111)\n","plot_flattened_dynamics(model, pca_model, xhat, ax=ax)\n","plt.title('fit flow field');"]},{"cell_type":"markdown","metadata":{"id":"Nnda0wDnJonc"},"source":["### üê∏ More questions...\n","Once you get things working, you can experiment- how does your fit model change if you alter:  \n","* the number of observed \"neurons\" `ncells`?\n","* the amount of observed data `tmax`?\n","* the observation noise `sigma`?  \n","* the observation nonlinearity `transform`?  "]},{"cell_type":"markdown","metadata":{"id":"VNym-vKsJonc"},"source":["### 1.3.5 üèÉ Choosing the number of discrete states\n","\n","One fit model is nice, but we had to make a couple guesses to get there- in particular, the number of discrete states $K$.\n","\n","Rather than guessing a value of $K$, a more principled approach is to fit models with 1, 2, 3... states and compare the ELBO for each state count. Modify the code below to perform this fitting:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CbP1mdL8Jonc"},"outputs":[],"source":["# Global parameters\n","D_obs = raster.shape[1]  # number of observed dimensions\n","D_latent = 3             # number of latent dimensions\n","k_range = range(1,6)     # range of K's to test\n","\n","# initialize some variables to store info from the fitted models\n","models     = [0]*len(k_range)  # store the fitted models\n","elbos      = [0]*len(k_range)  # store the ELBO at each iteration\n","posteriors = [0]*len(k_range)  # store the posterior objects\n","\n","# and fit! this takes a few minutes per K, so go stretch your legs and get yourself some coffee\n"]},{"cell_type":"markdown","metadata":{"id":"mjR4p9gRJond"},"source":["Having fit all those models, the next step is to compare them and settle on a value of $K$! (Note, in practice this can be a bit noisy so you may end up running multiple iterations for each $K$ and taking the best one.)\n","\n","How do you decide which model is best? Make some plots below and pick your favorite fit."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PoJZOGiQJond"},"outputs":[],"source":["# look at ELBOs for each of your fit models!"]},{"cell_type":"markdown","metadata":{"id":"nGcQ_eIzJond"},"source":["### Visualizing observed and inferred systems\n","As above, we'll use the `plot_trajectory` and `plot_flattened_dynamics` helper functions to see the inferred states of our best-fit model."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"y3CXs8oWJond"},"outputs":[],"source":["use_K = _   # pick your favorite model!\n","\n","# everything below here you can run as-is\n","\n","# get the inferred states and latent dimensions from your fit model\n","model = models[k_range.index(use_K)]\n","xhat = posteriors[k_range.index(use_K)].mean_continuous_states[0]\n","zhat = model.most_likely_states(xhat, raster)\n","\n","# visualizing a high-D system is messy, so let's just look at the first 2 PC's:\n","# get the first 2 PC's of the \"neural\" data\n","pca_data   = PCA(n_components=2)\n","pcs_data   = pca_data.fit_transform(raster)\n","\n","# get the first 2 PC's of the latent dimensions of the fit model\n","pca_model = PCA(n_components=2)\n","pcs_model   = pca_model.fit_transform(xhat)\n","\n","# and plot everything!\n","fig = plt.figure(figsize=(8,4))\n","\n","# plot the original system\n","ax = fig.add_subplot(121)\n","plt.plot(pcs_data[:,0], pcs_model[:,1], lw=1)\n","plt.title('first 2 PCs of data')\n","\n","# plot the inferred latent trajectories and states\n","ax2 = fig.add_subplot(122)\n","plot_trajectory(zhat, pcs_model, ax=ax2)\n","plt.title('first 2 PCs of fit latents')\n","\n","fig2 = plt.figure(figsize=(8,8))\n","ax = fig2.add_subplot(111)\n","plot_flattened_dynamics(model, pca_model, xhat, ax=ax)\n","plt.title('fit flow field')"]},{"cell_type":"markdown","metadata":{"id":"xcSiM70gJond"},"source":["### üê∏ Yet more questions...\n","Your bet-fit model and number of states may differ from your neighbors'- these systems are still sensitive to initial conditions. Some other experiments to try\n","* can you modify this code to use a different `transitions` model? How could you visualize that output?\n","* does changing the `emissions` model help at all to correct for nonlinear observations of the model state?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BpynZBztJond"},"outputs":[],"source":["# once you're ready to move on to part 2, you'll probably want to clear your notebook kernel.\n","# here's a copy of the original import cell at the top of this notebook, since it has been a while.\n","\n","%matplotlib notebook\n","\n","import os\n","import pickle\n","import copy\n","\n","import autograd.numpy as np\n","import autograd.numpy.random as npr\n","npr.seed(12345)\n","\n","import matplotlib.pyplot as plt\n","from matplotlib import gridspec\n","from sklearn.decomposition import PCA\n","from scipy import linalg\n","\n","import seaborn as sns\n","color_names = [\"windows blue\", \"red\", \"amber\", \"faded green\"]\n","colors = sns.xkcd_palette(color_names)\n","sns.set_style(\"white\")\n","sns.set_context(\"talk\")\n","\n","import ssm"]},{"cell_type":"markdown","metadata":{"id":"oe-7f-OyJond"},"source":["# 2. üèéÔ∏è Input-driven dynamical systems ($\\dot{x} = f(\\mathbf{A}x + \\mathbf{V}u$))\n","\n","The `simulators` library also includes support for input-driven (i.e. non-homogeneous) dynamical systems. You can add inputs to an input-driven system with the following functions:\n","\n","#### `model.add_pulse(time, (amplitude, weights))`: Creates a brief pulse of width $10dt$ (dt is the simulation timestep).\n","> Inputs:\n","> * `time` specifies when the input pulse is delivered, in seconds.\n","> * `amplitude` (default 1) sets pulse amplitude. This gets scaled by $1/(10dt)$ to account for the simulation timestep.\n","> * `weights` (default 1 to all units) sets the input weight onto the dimensions of the system.\n","\n","#### `model.add_step(time, (time_off, amplitude, weights))`: Creates a step input.\n","> Inputs:\n","> * `time` specifies when the input step turns on, in seconds.\n","> * `time_off` (default $\\infty$) specifies when the input step turns off, in seconds.\n","> * `amplitude` (default 1) sets step amplitude. Unlike `add_pulse`, the step amplitude is not scaled by dt.\n","> * `weights` (default 1 to all units) sets the input weight onto the dimensions of the system.\n","\n","#### `model.add_input_noise(amplitude, (weights))`: Creates an ongoing white-noise input.\n","> Note, there is no onset time here, as noise is added to every step of the simulation.\n","> Inputs:\n","> * `amplitude` specifies the variance of the noise; this is scaled by $\\sqrt{1/dt}$ to make units a bit more similar to those of the input pulses.\n","> * `weights` (default 1 to all units) sets the weight onto the dimensions of the system.\n","\n","#### `model.clear_inputs()` removes all added inputs from the model.\n","> If inputs aren't removed, they'll continue to be applied for all repeat simulations, even if persistent=True in your simulation call.\n","\n","#### `model.plot_simulation()` modified.\n","> This function behaves as before, but now includes the input to the model plotted below the state variables.\n","\n","### The `simulators.RNN` model\n","To gain some intuition for how state space models work with neural data, let's try running them on some simulated data! The `RNN` model adds just a few additional elements on top of the standard input-driven model:\n","\n","#### `simulators.RNN((n_dims))` model initialization.\n","> Optional input `n_dims` specifies the number of latent dimensions of the toy model.\n","> Note that if you initialize $\\mathbf{A}$ to be random, the behavior of the model will be a bit more stable if n_dims is a bit larger (e.g. default is 20), however this doesn't mean that the system's true latent dimensionality is that large.\n",">\n","> Alternatively, make `n_dims` smaller and make $\\mathbf{A}$ upper-triangular or diagonal for a system that's a bit easier to work with.\n","\n","#### `model.A` is the dynamics matrix.\n","> You can reassign the values of $A$  to change the dynamics of the system.\n","\n","#### `model.dynamics(time, (X))` computes model dynamics.\n","> Inputs are the same in the autonomous model, but now the dynamics have the form  $ \\dot{x} = f( \\mathbf{A} x + \\textrm{input}) $, where `input` consists of the pulses, steps, and noise terms you add to the model.\n","\n","#### `model.set_nonlinearity(nonlin)` applies a nonlinearity to the dynamics.\n","> * `nonlin` is a string or a `lambda`; currently supports these options:\n",">     * `'linear'`: f(x) = x\n",">     * `'nonneg'`: f(x) = max(x,0)\n",">     * `'tanh'`: f(x) = tanh(x)\n",">     * or you can pass a python `lambda` of your own design."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mgw0H61aJone"},"outputs":[],"source":["%matplotlib notebook\n","import simulators\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","n_dims = 20\n","integrator = simulators.RNN(n_dims=n_dims)\n","\n","# let's create a dynamics matrix with one integration dimension\n","# drawing Gaussian-distributed weights with a variance that scales with 1/sqrt(n_dims)\n","# ensures that scaling up or down n_dims doesn't dramatically change our system (in the\n","# limit where n_dims is large.)\n","integrator.A = np.random.normal(0, 0.5/np.sqrt(n_dims), (n_dims, n_dims))\n","integrator.A[0,0] = 0.975  # make an integration dimension in our dynamics matrix\n","\n","\n","integrator.set_nonlinearity('linear')"]},{"cell_type":"markdown","metadata":{"id":"yi6MEWILJone"},"source":["### 2.1.1 Getting to know the dynamics matrix\n","Recall that our model's dynamics matrix $\\mathbf{A}$ determines how our system behaves (if it's linear). Before we do any simulation, let's take a look at the eigenvalues of the dynamics matrix we constructed above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0SMNFPIcJone"},"outputs":[],"source":["e, v = np.linalg.eig(integrator.A)\n","\n","fig = plt.figure(figsize=(5,5))\n","ax = fig.add_subplot(111)\n","plt.plot(e.real, e.imag,'.')\n","plt.xlabel('real part')\n","plt.ylabel('imaginary part')\n","plt.axvline(x=1, color='r', linestyle='--');"]},{"cell_type":"markdown","metadata":{"id":"PFNwhyrWJoni"},"source":["ü§î What does the red line represent?\n","\n","If these eigenvalues tell us the system will be unstable, we can re-generate the dynamics matrix, and perhaps reduce its variance.\n","\n","We can also check the **trace** and **determinant** of our matrix to predict more about its behavior. Do so below. what do these values tell you?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pr_bWQKIJoni"},"outputs":[],"source":["# fill in equations for the trace, determinant, and largest eigenvalue:\n","\n","print('the trace is ' + '{:.5}'.format(___))\n","print('the determinant is ' + '{:.5}'.format(___))\n","print('the largest eigenvalue is ' + '{:.5}'.format(___))"]},{"cell_type":"markdown","metadata":{"id":"NmfDcEleJoni"},"source":["Having reflected upon that, let's get to simulating.\n","\n","You can drive your model with whatever signals you'd like - to start with, I'm giving it a three-dimensional noise input, two brief pulses of input to all units, and a five-second step input just to the integrator unit."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rE10n189Joni"},"outputs":[],"source":["fig = plt.figure(figsize=(10,5))\n","ax = fig.add_subplot(121, projection='3d', aspect='auto')\n","\n","integrator.clear_inputs()  # so that re-running this cell doesn't add inputs a second time\n","\n","integrator.add_input_noise(0.1, weights=np.random.normal(0,1,(n_dims)))\n","integrator.add_input_noise(0.1, weights=np.random.normal(0,1,(n_dims)))\n","integrator.add_input_noise(0.1, weights=np.random.normal(0,1,(n_dims)))\n","integrator.add_pulse( 3, amplitude=1)\n","integrator.add_pulse(10, amplitude=1)\n","\n","# add an input to the integrator\n","in_wts = np.zeros(n_dims)\n","in_wts[0] = 1\n","integrator.add_step( 20, time_off=25, amplitude=1.5, weights=in_wts)\n","\n","integrator.plot_simulation(tmax=100, ax=ax, pca=True)\n","\n","ax2 = fig.add_subplot(122)\n","integrator.plot_timeseries(ax=ax2, normalize=True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rUvouC6GJonj"},"source":["As before, you can make neurons that are noisy observations of your dynamical system:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DD6-BrxrJonj"},"outputs":[],"source":["ncells = 200  # however many you like\n","raster = integrator.observe_simulation(ncells, tmax=100, sigma=3)\n","\n","fig = plt.figure(figsize=(7,5))\n","ax = fig.add_subplot(111)\n","plt.imshow(raster.T, aspect='auto')\n","plt.colorbar();"]},{"cell_type":"markdown","metadata":{"id":"y78WqO-pJonj"},"source":["### 2.1.2 üèÉ Picking the size of the fit dynamical system\n","\n","Our integrator model has 20 dynamical units- but is it a 20-dimensional system? Use PCA on the activity you generated above to decide how many latent dimensions you need to \"adequately\" capture what's going on in the model.\n","\n","ü§î Should you run PCA on the observed activity or the latents? Does it matter?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-fLEY5URJonj"},"outputs":[],"source":["# look at the dimensionality of your observed dynamical system- perhaps with PCA?"]},{"cell_type":"markdown","metadata":{"id":"mu3LUwmTJonj"},"source":["## 2.1.3 üèÉ Fitting the input-driven rSLDS model\n","If you've picked a number of latent dimensions you're happy with, now is the time to fit your model. We just need to pull the model input from our `integrator` object. This gets passed as an additional argument `inputs` in two places: `ssm.initialize` and `ssm.fit`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJb-7qOOJonj"},"outputs":[],"source":["# Global parameters you'll need to set:\n","D_obs = _                # number of observed dimensions\n","D_latent = _             # number of latent dimensions - up to you!\n","K = _                    # number of latent states to fit - up to you!\n","M = _                    # number of input dimensions - up to you!\n","\n","\n","# re-generate the input that was fed to our model, without noise.\n","timestamps = range(0, len(integrator.last_sim[0]))\n","in_sig = np.array([integrator._input(t*integrator.dt, noise=False) for t in timestamps])\n","\n","# assuming you kept my example inputs, we can just keep the input to the first 2 units\n","model_input = in_sig[:,:2]\n","\n","\n","# Now fit that model:\n","# note that we have a new parameter M that should be used when initializing the SLDS model, using a keyword argument (M=M)"]},{"cell_type":"markdown","metadata":{"id":"vrLm8lc6Jonj"},"source":["## 2.1.4 üèÉ Plotting the ELBO and flow field\n","\n","As for the Lorenz system, we should check our ELBOs to make sure the model has converged. If it has, then we can go ahead and examine the fit system."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sSNc4HQjJonk"},"outputs":[],"source":["# you know the drill- plot that ELBO"]},{"cell_type":"markdown","metadata":{"id":"wCUv5fPaJonk"},"source":["How should we visualize the system? Here are a couple options, but can you think of others?"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"czrpMhdPJonk"},"outputs":[],"source":["# get the values of xhat and zhat\n","xhat = _\n","zhat = _\n","\n","# everything below here you can run as-is\n","# get the first 2 PC's of the \"neural\" data\n","pca_data   = PCA(n_components=2)\n","pcs_data   = pca_data.fit_transform(raster)\n","\n","# get the first 2 PC's of the latent dimensions of the fit model\n","pca_model  = PCA(n_components=2)\n","pcs_model  = pca_model.fit_transform(xhat)\n","\n","\n","fig = plt.figure(figsize=(8,4))\n","\n","# plot the original system\n","ax = fig.add_subplot(121)\n","plt.plot(pcs_data[:,0], pcs_data[:,1], lw=1)\n","plt.title('first 2 PCs of data')\n","\n","# plot the inferred latent trajectories and states\n","ax2 = fig.add_subplot(122)\n","plot_trajectory(zhat, pcs_model, ax=ax2)\n","plt.title('most-likely state of fit latents')\n","\n","\n","# Make some plots to compare the PCs of the observed data to those of the fit system:\n","fig = plt.figure(figsize=(8,8))\n","ax2 = fig.add_subplot(211)\n","plt.plot(pcs_data[:,0], label='data')\n","plt.plot(pcs_model[:,0], label='fit')\n","plt.legend()\n","ax2 = fig.add_subplot(212)\n","plt.plot(pcs_data[:,1], label='data')\n","plt.plot(pcs_model[:,1], label='fit')\n","plt.legend()\n","\n","# Finally, show the fit flow field of the model:\n","fig2 = plt.figure(figsize=(8,8))\n","ax = fig2.add_subplot(111)\n","plot_flattened_dynamics(model, pca_model, xhat, ax=ax)\n","plt.title('fit flow field');"]},{"cell_type":"markdown","metadata":{"id":"y-G72wbEJonk"},"source":["### 2.1.5 üèÉ Getting the inputs wrong\n","\n","In the fits above, we knew exactly what the input to our model was- as a result, our inferred flow field mostly ignores the input-driven portions of the system trajectories (ü§î how can you see that in the flow field?)\n","\n","üê∏ What would happen if we didn't know the model inputs, or if we only knew a subset of them? what if we guessed the wrong format of the inputs? Try going back to your model above, and re-fitting with missing, incomplete, or incorrect inputs. What kind of effect does this have on the inferred model dynamics?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAVKZirzJonk"},"outputs":[],"source":["# insert fitting and visualization code here (or just go back up and re-run the cells above)"]},{"cell_type":"markdown","metadata":{"id":"dm-MxcC0Jonl"},"source":["### 2.1.6 üèÉ Examining parameters of the fit model\n","\n","Once you're happy with your model fit, you can dig into its components to study them in more detail.\n","Recall that our fit model has the form\n","\n","$$\n","x_t = A_k x_{t-1} + V_k u_{t} + b_k\n","$$\n","\n","the fit dynamics matrix $\\mathbf{A}$, input weights $\\mathbf{V}$, and offset term $\\mathbf{b}$ of our model can be unpacked from your model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6MGsPTXAJonl"},"outputs":[],"source":["print(model.dynamics.As)\n","print(model.dynamics.bs)\n","print(model.dynamics.Vs)"]},{"cell_type":"markdown","metadata":{"id":"S7006MZVJonl"},"source":["How do the eigenvalues of your fit dynamics matrix (or matrices) compare to those of the original system?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"042ftQ4AJonl"},"outputs":[],"source":["e, v = np.linalg.eig(integrator.A)\n","e2, v2 = __  # compute the eigenvalues and eigenvectors of your fit dynamics matrix here\n","\n","# fill in the blanks:\n","print('the trace is ' + '{:.5}'.format(_))\n","print('the determinant is ' + '{:.5}'.format(_))\n","print('the largest eigenvalue of the model is ' + '{:.5}'.format(_))\n","print('the largest eigenvalue of the fit is ' + '{:.5}'.format(_))\n","\n","fig = plt.figure(figsize=(5,5))\n","ax = fig.add_subplot(111)\n","plt.plot(e.real, e.imag,'.')\n","plt.plot(e2.real, e2.imag,'r*')\n","plt.xlabel('real part')\n","plt.ylabel('imaginary part')\n","plt.axvline(x=1, color='r', linestyle='--');"]},{"cell_type":"markdown","metadata":{"id":"rH4Izc2bJonl"},"source":["## 2.2 üê∏ further RNN adventures\n","\n","If we've made it this far, then things went well. Maybe too well?\n","\n","Obviously we've only scratched the surface of things you can do with input-driven dynamical systems. And you may have noticed above that the eigenvalues of the fit dynamics matrix aren't quite capturing the dynamics of the full original system. What might improve this? What might make the fit worse?\n","\n","Some ideas for iterating on the above model:\n","* Make a hand-designed dynamics matrix $\\mathbf{A}$ of your neural system, by picking a point you want to simulate from the Poincar√© diagram.\n","* Add nonlinearities to the RNN to introduce oscillatory or chaotic dynamics. How does the rSLDS handle these?\n","* Write a new input generation function for `simulators.driven` to allow you to more easily feed time series data into your RNN model.\n","* Train an RNN on a task (eg using FORCE) and then compare its dynamics and eigenvalues to those of a fit rSLDS.\n","* And of course, try fitting on some real neural data.\n","\n","<div align=center>\n","    ü§ñü§ñü§ñ Happy simulating! ü§ñü§ñü§ñ\n","    </div>"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":0}